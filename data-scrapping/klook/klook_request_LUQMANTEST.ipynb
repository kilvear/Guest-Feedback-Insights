{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import random\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import time\n",
    "import pandas as pd\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_user_agents(filepath):\n",
    "  # Load user agents list from a txt file\n",
    "  file = open(filepath, \"r\")\n",
    "  agents = file.read()\n",
    "  user_agent_list = agents.split(\"\\n\")\n",
    "  file.close()\n",
    "  return user_agent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent_list = load_user_agents(\"user-agents_chrome_browser_100-0.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get free proxies\n",
    "\n",
    "res = requests.get('https://free-proxy-list.net')\n",
    "content = bs(res.text, 'lxml')\n",
    "table = content.find('table')\n",
    "rows = table.find_all('tr')\n",
    "cols = [[col.text for col in row.find_all('td')] for row in rows]\n",
    "\n",
    "proxieslist = []\n",
    "\n",
    "for col in cols:\n",
    "  try:\n",
    "    if col[4] == 'elite proxy' and col[6] == 'yes':\n",
    "      proxieslist.append('https://' + col[0] + ':' + col[1])\n",
    "  except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxieslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run every 5 minutes to be safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attraction = 'adventure_cove'\n",
    "file_path = 'adventurecovetest.csv'\n",
    "\n",
    "# Open csv file to write in\n",
    "with open(file_path, 'a', encoding=\"utf-8\", newline='') as f:\n",
    "  csv_writer = csv.writer(f)\n",
    "  csv_writer.writerow(['date', 'user', 'review', 'attraction', 'source'])\n",
    "\n",
    "page = 0\n",
    "count_reviews = 0\n",
    "\n",
    "while True:\n",
    "  try:\n",
    "    page += 1\n",
    "    url = f'https://www.klook.com/v1/usrcsrv/activities/120/reviews?page={page}&limit=8&star_num=&lang=id_ID&sort_type=1&only_image=false'\n",
    "\n",
    "    for _ in user_agent_list:\n",
    "      #Pick a random user agent\n",
    "      user_agent = random.choice(user_agent_list)\n",
    "      #Set the headers \n",
    "      headers = {'User-Agent': user_agent, 'referer': \"https://www.google.com\"}\n",
    "\n",
    "    time.sleep(randrange(10,30))\n",
    "    source = requests.get(url, headers=headers)\n",
    "    \n",
    "    data = source.json()\n",
    "\n",
    "    content = data['result']['item']\n",
    "\n",
    "    review_count = len(content)\n",
    "    \n",
    "    for i in range(review_count):\n",
    "      date = content[i]['date']\n",
    "      author = content[i]['author']\n",
    "      review = content[i]['content']\n",
    "      print(date, author, review)\n",
    "      \n",
    "      # Open csv file to write in\n",
    "      with open(file_path, 'a', encoding=\"utf-8\", newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerow([date, author, review, attraction, 'klook']) \n",
    "  except:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape in Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_klook(attraction, id):\n",
    "  file_path = f'{attraction.lower().replace(\" \", \"_\")}.csv'\n",
    "\n",
    "  # Open csv file to write headers\n",
    "  with open(f'data/raw/{file_path}', 'a', encoding=\"utf-8\", newline='') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerow(['date', 'user', 'review', 'attraction', 'source'])\n",
    "\n",
    "  page = 0\n",
    "  # count_reviews = 0\n",
    "\n",
    "  while True:\n",
    "    try:\n",
    "      page += 1\n",
    "      url = 'https://www.klook.com/v1/usrcsrv/activities/' + str(id) + '/reviews?page=' + str(page) + '&limit=8&star_num=&lang=id_ID&sort_type=1&only_image=false'\n",
    "\n",
    "      for _ in user_agent_list:\n",
    "        #Pick a random user agent\n",
    "        user_agent = random.choice(user_agent_list)\n",
    "        #Set the headers \n",
    "        headers = {'User-Agent': user_agent, 'referer': \"https://www.google.com\"}\n",
    "\n",
    "      time.sleep(randrange(60,120))\n",
    "      source = requests.get(url, headers=headers)\n",
    "      \n",
    "      data = source.json()\n",
    "      print(data)\n",
    "\n",
    "      content = data['result']['item']\n",
    "\n",
    "      review_count = len(content)\n",
    "      \n",
    "      for i in range(review_count):\n",
    "        date = content[i]['date']\n",
    "        author = content[i]['author']\n",
    "        review = content[i]['content']\n",
    "        print(date, author, review)\n",
    "        \n",
    "        # Open csv file to write in\n",
    "        with open(f'data/raw/{file_path}', 'a', encoding=\"utf-8\", newline='') as f:\n",
    "          csv_writer = csv.writer(f)\n",
    "          csv_writer.writerow([date, author, review, attraction, 'klook']) \n",
    "    except:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('attractions_id.csv', header=None, names=[\"Attraction\", \"ID\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "  time.sleep(randrange(300,420))\n",
    "  scrape_klook(row['Attraction'], row['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_klook('test', 139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90b85f54aecd71003771950f482cb0e9b344c8b39504f9f49f23fe15bb6ae351"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
